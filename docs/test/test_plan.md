# テスト計画

このドキュメントでは、AWS リソースコスト報告ツールのテスト戦略と実施計画について説明します。

## テスト戦略

### テストの目的

- 各コンポーネントが期待通りに動作することを確認する
- コンポーネント間の連携が正しく行われることを検証する
- エラー処理が適切に機能することを確認する
- パフォーマンス要件が満たされていることを検証する
- 将来的な拡張性と保守性を確保する

### テストレベル

1. **ユニットテスト**: 個々のクラスやメソッドの機能を検証
2. **統合テスト**: コンポーネント間の連携を検証
3. **エンドツーエンドテスト**: システム全体の動作を検証
4. **パフォーマンステスト**: 大規模データでの性能を検証

### テスト環境

- **開発環境**: ローカル開発環境（モックデータを使用）
- **テスト環境**: テスト用AWSアカウント（限定的なリソース）
- **本番環境**: 実際の運用環境（実データを使用）

## テスト対象コンポーネント

### 1. データ収集コンポーネント

- **テスト対象**:
  - AWSResourceCollector クラス
  - 各サービス専用の Collector クラス

- **テスト内容**:
  - AWS APIとの正しい通信
  - ページング処理の正確さ
  - エラー処理と再試行ロジック
  - タグ取得処理
  - 各サービスに特化した収集ロジック

### 2. データ処理コンポーネント

- **テスト対象**:
  - DataProcessor クラス

- **テスト内容**:
  - 生データの保存機能
  - サマリー情報の生成
  - トレンドデータの生成
  - 変更検出機能
  - コスト計算ロジック

### 3. レポート生成コンポーネント

- **テスト対象**:
  - ReportGenerator クラス

- **テスト内容**:
  - サマリーレポート生成
  - トレンドレポート生成
  - 変更レポート生成
  - コストレポート生成
  - グラフ生成機能
  - マークダウンからHTMLへの変換

### 4. エクスポートコンポーネント

- **テスト対象**:
  - CSVExporter クラス
  - JSONExporter クラス

- **テスト内容**:
  - 各形式への正確なエクスポート
  - 文字エンコーディング処理
  - 大規模データの処理

## テスト手法

### ユニットテスト

- **実装手法**: PyTestフレームワークを使用
- **モックの使用**: AWS APIの呼び出しをモック化
- **テストカバレッジ**: 最低80%のコードカバレッジを目標

```python
# DataProcessorのユニットテスト例
def test_generate_summary():
    processor = DataProcessor(base_dir=test_dir)
    summary_file = processor.generate_summary("2025-02-01")
    
    # ファイルが作成されたことを確認
    assert os.path.exists(summary_file)
    
    # 内容を検証
    with open(summary_file, 'r') as f:
        data = json.load(f)
        assert data['metadata']['source_date'] == "2025-02-01"
        assert 'resource_summary' in data
```

### 統合テスト

- **実装手法**: スクリプト化されたテストシナリオ
- **テストデータ**: 模擬データセットを使用
- **検証項目**: コンポーネント間の連携が正しく行われること

```python
# 統合テスト例
def test_end_to_end_workflow():
    # 1. リソース収集（モック）
    resources = mock_resource_data()
    
    # 2. データ処理
    processor = DataProcessor(base_dir=test_dir)
    date_dir = processor.save_raw_data(resources)
    summary_file = processor.generate_summary()
    
    # 3. レポート生成
    generator = ReportGenerator(base_dir=test_dir)
    report_file = generator.generate_summary_report()
    
    # 4. 結果の検証
    assert os.path.exists(report_file)
    with open(report_file, 'r') as f:
        content = f.read()
        assert "AWS リソースサマリーレポート" in content
```

### エンドツーエンドテスト

- **実装手法**: CLI実行による検証
- **テスト環境**: テスト用AWSアカウント
- **検証項目**: 実際のAWS環境での正確な動作

```bash
# エンドツーエンドテスト例
python src/main.py --profile test-account --region ap-northeast-1
```

### パフォーマンステスト

- **実装手法**: 大規模リソースセット（模擬）を使用
- **測定項目**: 実行時間、メモリ使用量、CPU使用率
- **基準値**: 5000リソース以上での処理時間が5分以内

## テストケース

### ユニットテストケース

| ID | テストケース | 期待結果 | 優先度 |
|----|-------------|---------|-------|
| UT-DP-01 | DataProcessor: 生データ保存 | ファイルが正しく作成され、内容が検証可能 | 高 |
| UT-DP-02 | DataProcessor: サマリー生成 | サマリーJSONが正しく生成される | 高 |
| UT-DP-03 | DataProcessor: トレンドデータ生成 | トレンドファイルが正しく生成される | 中 |
| UT-DP-04 | DataProcessor: 変更レポート生成 | 変更が正しく検出され、レポートが生成される | 高 |
| UT-RG-01 | ReportGenerator: サマリーレポート | マークダウンレポートが正しく生成される | 高 |
| UT-RG-02 | ReportGenerator: HTML変換 | マークダウンからHTMLへの変換が正しく行われる | 中 |
| UT-RG-03 | ReportGenerator: グラフ生成 | グラフ画像が正しく生成される | 中 |

### 統合テストケース

| ID | テストケース | 期待結果 | 優先度 |
|----|-------------|---------|-------|
| IT-01 | 収集→処理のワークフロー | データが収集され、処理される | 高 |
| IT-02 | 処理→レポート生成のワークフロー | 処理済みデータからレポートが生成される | 高 |
| IT-03 | レポート→HTML変換のワークフロー | マークダウンレポートがHTMLに変換される | 中 |
| IT-04 | コマンドラインオプション | 各オプションが期待通りに動作する | 高 |

### エンドツーエンドテストケース

| ID | テストケース | 期待結果 | 優先度 |
|----|-------------|---------|-------|
| E2E-01 | デフォルト実行 | すべてのリソースが収集され、レポートが生成される | 高 |
| E2E-02 | 特定リージョン指定実行 | 指定リージョンのリソースのみが収集される | 中 |
| E2E-03 | 特定プロファイル指定実行 | 指定アカウントのリソースが収集される | 中 |
| E2E-04 | レポート形式指定実行 | 指定された形式のレポートのみが生成される | 中 |

## 品質基準

- **バグ重大度の定義**:
  - **Critical**: システム全体が機能しない
  - **Major**: 主要機能が正常に動作しない
  - **Minor**: 一部の機能に問題があるが、代替手段がある
  - **Cosmetic**: UI/UXに関する小さな問題

- **テスト合格基準**:
  - Critical バグがゼロであること
  - Major バグがゼロであること
  - Minor バグが3個以下であること
  - コードカバレッジが80%以上であること

## テスト実行手順

### 1. 開発環境でのテスト実行

```bash
# ユニットテスト
pytest tests/unit/

# 統合テスト
pytest tests/integration/

# カバレッジレポート生成
pytest --cov=src tests/
```

### 2. テスト環境でのエンドツーエンドテスト

```bash
# テスト用AWSアカウントでの実行
python src/main.py --profile test-account --region ap-northeast-1

# 特定のレポートのみ生成
python src/main.py --profile test-account --report summary
```

### 3. パフォーマンステスト

```bash
# リソース数を段階的に増やしながら実行時間を計測
python performance_test.py --resource-count 1000
python performance_test.py --resource-count 5000
python performance_test.py --resource-count 10000
```

## テスト結果報告

テスト結果は以下の形式で文書化し、プロジェクト管理者に報告します：

- **テスト概要**: 実施したテストの種類と範囲
- **テスト結果サマリー**: 合格/不合格の件数
- **発見されたバグ**: 重大度別のバグリスト
- **パフォーマンス測定結果**: 実行時間、リソース使用量
- **改善提案**: テスト中に発見された改善点
- **カバレッジレポート**: コードカバレッジの詳細

## 更新履歴

| 日付 | バージョン | 説明 | 作成者 |
|------|------------|------|--------|
| 2025-03-27 | 1.0 | 初期バージョン | - |
